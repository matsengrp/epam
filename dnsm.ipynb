{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from epam.dnsm import train_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp_df = pd.read_csv(\"~/data/wyatt-10x-1p5m_pcp_2023-10-07.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing data...\n",
      "training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/torch/nn/modules/transformer.py:380: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.34231334924697876, Validation Loss: 0.3454280197620392\n",
      "Epoch [2/10], Training Loss: 0.3213242292404175, Validation Loss: 0.33188706636428833\n",
      "Epoch [3/10], Training Loss: 0.3087613880634308, Validation Loss: 0.32142314314842224\n",
      "Epoch [4/10], Training Loss: 0.30056893825531006, Validation Loss: 0.3096000552177429\n",
      "Epoch [5/10], Training Loss: 0.30200135707855225, Validation Loss: 0.29866546392440796\n",
      "Epoch [6/10], Training Loss: 0.264438271522522, Validation Loss: 0.2887866199016571\n",
      "Epoch [7/10], Training Loss: 0.24836869537830353, Validation Loss: 0.27943721413612366\n",
      "Epoch [8/10], Training Loss: 0.26803213357925415, Validation Loss: 0.27047407627105713\n",
      "Epoch [9/10], Training Loss: 0.2465275675058365, Validation Loss: 0.26193925738334656\n",
      "Epoch [10/10], Training Loss: 0.2605518102645874, Validation Loss: 0.25384682416915894\n"
     ]
    }
   ],
   "source": [
    "nhead = 4\n",
    "dim_feedforward = 2048\n",
    "layer_count = 3\n",
    "model = train_model(pcp_df, nhead=nhead, dim_feedforward=dim_feedforward, layer_count=layer_count, batch_size=32, num_epochs=10, learning_rate=0.001, checkpoint_dir=\"./_checkpoints\", log_dir=\"./_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Hyperparameters and Data Preparation\n",
    "n_sequences = 10000  # Number of sequences\n",
    "sequence_length = 20  # Length of each sequence\n",
    "n_aa = 20  # Number of amino acids\n",
    "\n",
    "# Define polar amino acids by their indices in AA_STR_SORTED = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "polar_aa_indices = [10, 7, 3, 11, 4, 8, 9, 15, 16, 19]\n",
    "\n",
    "# Randomly generate parent sequences\n",
    "parent_sequences = torch.randint(0, n_aa, (n_sequences, sequence_length))\n",
    "\n",
    "# Create mutation masks based on polar amino acids\n",
    "mutation_masks = torch.zeros((n_sequences, sequence_length))\n",
    "\n",
    "for i in range(1, sequence_length - 1):\n",
    "    is_prev_polar = torch.any(torch.eq(parent_sequences[:, i - 1].unsqueeze(1), torch.tensor(polar_aa_indices)), dim=1)\n",
    "    is_next_polar = torch.any(torch.eq(parent_sequences[:, i + 1].unsqueeze(1), torch.tensor(polar_aa_indices)), dim=1)\n",
    "    mutation_masks[:, i] = is_prev_polar | is_next_polar\n",
    "\n",
    "# One-hot encode parent sequences\n",
    "parent_sequences = F.one_hot(parent_sequences, num_classes=n_aa).float()\n",
    "\n",
    "# Convert to float and add some noise\n",
    "mutation_masks = mutation_masks.float()\n",
    "mutation_masks += torch.randn_like(mutation_masks) * 0.1  # adding noise\n",
    "\n",
    "# Clip to [0, 1] range\n",
    "mutation_masks = torch.clamp(mutation_masks, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matsen/mambaforge/envs/epam/lib/python3.9/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Transformer Model\n",
    "class MutationPredictor(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers):\n",
    "        super(MutationPredictor, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers)\n",
    "        self.linear = nn.Linear(d_model, 1)  # Linear layer to output probabilities\n",
    "\n",
    "    def forward(self, src):\n",
    "        out = self.encoder(src.transpose(0, 1))\n",
    "        out = self.linear(out)\n",
    "        return torch.sigmoid(out).transpose(0, 1).squeeze(-1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "d_model = n_aa  # This assumes that your one-hot encoding size matches d_model\n",
    "nhead = 4\n",
    "num_layers = 2\n",
    "model = MutationPredictor(d_model, nhead, num_layers)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training Loop\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(parent_sequences)\n",
    "    loss = criterion(output, mutation_masks)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
