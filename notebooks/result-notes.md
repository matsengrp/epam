## hydrophobic neighbor mutator simulation

```
nhead = 4
dim_feedforward = 2048
layer_count = 3

Without	the positional encoder:
Epoch [0/2], Training Loss: 5.478170156478882, Validation Loss: 5.341393775448075
Epoch [1/2], Training Loss: 0.22140918672084808, Validation Loss: 0.1811294092190983
Epoch [2/2], Training Loss: 0.22316762804985046, Validation Loss: 0.18113862033559805

With positional encoder:
Epoch [0/2], Training Loss: 6.603643817915294, Validation Loss: 6.601628321289675
Epoch [1/2], Training Loss: 0.17955662310123444, Validation Loss: 0.18135988180736415
Epoch [2/2], Training Loss: 0.12157808989286423, Validation Loss: 0.181174641160193


nhead = 2
dim_feedforward = 256
layer_count = 2

Epoch [0/2], Training Loss: 6.370150334865838, Validation Loss: 6.379060747971849
Epoch [1/2], Training Loss: 0.23558320105075836, Validation Loss: 0.18132765149233335
Epoch [2/2], Training Loss: 0.21081417798995972, Validation Loss: 0.18122180218255965

nhead = 1
dim_feedforward = 16
layer_count = 1

Without	the positional encoder:
Epoch [0/2], Training Loss: 1.877058620859937, Validation Loss: 1.8647038502816142
Epoch [1/2], Training Loss: 0.16272467374801636, Validation Loss: 0.18125704404788578
Epoch [2/2], Training Loss: 0.21040047705173492, Validation Loss: 0.18125464423508902

With positional encoder:
Epoch [0/2], Training Loss: 1.678296947838415, Validation Loss: 1.6787940547254503
Epoch [1/2], Training Loss: 0.1890346258878708, Validation Loss: 0.1817096910068504
Epoch [2/2], Training Loss: 0.21665123105049133, Validation Loss: 0.1816061288331846
```

## training does something for a dnsm

Without training, predictions for selection factors look like

```
array([1.2295574, 1.2999198, 1.3371192, 1.3150158, 1.2597741, 1.2435262,
       1.2842144, 1.3460298, 1.377234 , 1.3423512, 1.2746744, 1.2338479,
       1.2429193, 1.2858554, 1.3255156, 1.3160247, 1.2609961, 1.214182 ,
       1.2182986, 1.2666048, 1.3183944, 1.3242812, 1.2685162, 1.2022132,
       1.1864163, 1.2209891, 1.2590964, 1.2687483, 1.231885 , 1.1734569,
       1.1408241, 1.1626495, 1.2194378, 1.2707843, 1.2747616, 1.2311916,
       1.1961764, 1.219443 , 1.2791331, 1.3255329, 1.3193125, 1.269207 ,
       1.2177291, 1.2089113, 1.2369877, 1.2769942, 1.2880511, 1.2541292,
       1.2078887, 1.1953306, 1.2333612, 1.2877362, 1.3138413, 1.2823462,
       1.2230933, 1.191793 , 1.2086565, 1.2402045, 1.2504033, 1.2209702,
       1.1630723, 1.1212176, 1.1231791, 1.1564173, 1.1954066, 1.2057782,
       1.1743045, 1.136235 , 1.1335902, 1.1693573, 1.2143885, 1.2272743,
       1.1981941, 1.1509302, 1.1273017, 1.1408833, 1.1797723, 1.2090954,
       1.2020926, 1.1684523, 1.1474736, 1.1672757, 1.2241232, 1.2770569,
       1.2840089, 1.2476239, 1.2101884, 1.2113245, 1.2445349, 1.2777718,
       1.2801954, 1.2377877, 1.1878679, 1.1741008, 1.2003534, 1.2496203,
       1.2831779, 1.2617031, 1.2175899, 1.1942217, 1.214595 , 1.2614547,
       1.2927188, 1.2807429, 1.2326771, 1.1846119, 1.1806487, 1.2129161,
       1.2540767, 1.2713538, 1.2394059, 1.1982856, 1.1936879, 1.2402737,
       1.3084261, 1.3507415, 1.3355999, 1.2912463, 1.2688143, 1.2950326,
       1.3429717, 1.3763609, 1.3650202], dtype=float32)
```

becomes:


```
array([1.0123688, 1.0142995, 1.0164003, 1.0171195, 1.0163578, 1.0152637,
       1.015235 , 1.016575 , 1.0183314, 1.0191091, 1.0181016, 1.0160481,
       1.0147545, 1.015236 , 1.0168464, 1.0179814, 1.0174118, 1.0157024,
       1.0146466, 1.015261 , 1.0171756, 1.0188904, 1.0191028, 1.0177436,
       1.0161238, 1.015726 , 1.0168979, 1.0184935, 1.0189238, 1.0174738,
       1.015392 , 1.0146089, 1.0159858, 1.0183274, 1.0198556, 1.019657 ,
       1.0184885, 1.0179417, 1.0189458, 1.0210406, 1.0226865, 1.0226761,
       1.021146 , 1.019577 , 1.0195011, 1.0208968, 1.0225412, 1.0228125,
       1.0215883, 1.0201914, 1.0200827, 1.0213643, 1.0231118, 1.0239779,
       1.0232943, 1.0217488, 1.020736 , 1.0210705, 1.0223538, 1.0231177,
       1.0222025, 1.0200809, 1.0184276, 1.0184495, 1.019829 , 1.0211911,
       1.0212866, 1.0201281, 1.018907 , 1.0189116, 1.0201957, 1.0217034,
       1.0221218, 1.0209353, 1.0190976, 1.018141 , 1.0187653, 1.020163 ,
       1.0209303, 1.0202978, 1.0189056, 1.0182031, 1.0189984, 1.0207165,
       1.0220393, 1.0220674, 1.0209092, 1.0196959, 1.0195574, 1.0205724,
       1.0216445, 1.02155  , 1.0200006, 1.018129 , 1.0174998, 1.0184311,
       1.0198394, 1.0204424, 1.0196593, 1.0182577, 1.0175657, 1.0182279,
       1.0196483, 1.0205252, 1.0199145, 1.0180717, 1.0165291, 1.0164578,
       1.0176736, 1.018876 , 1.0188107, 1.0174514, 1.0161853, 1.016423 ,
       1.0180731, 1.020101 , 1.0210247, 1.0203915, 1.0190666, 1.0186397,
       1.019658 , 1.0214067, 1.022373 ], dtype=float32)
```


However, this doesn't change the loss much:

```
Epoch [0/2], Training Loss: 0.14409101641178132, Validation Loss: 0.1434180026408285
training model...
Epoch [1/2], Training Loss: 0.14379814267158508, Validation Loss: 0.14142218604683876
Epoch [2/2], Training Loss: 0.1677856147289276, Validation Loss: 0.14140618755482137
```